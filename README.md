# HippRAG Model

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

# ALL IN HF https://huggingface.co/xxzws/hipporag-triple

## Introduction / ä»‹ç»


**English:**  
This model is designed for extracting Chinese HippRAG triples (subject-predicate-object). It is trained exclusively on Chinese corpora but can be extended to simulate other languages via the provided training code. Based on Qwen3-1.7B, it supports extension to other models in the Qwen series; compatibility with other model families remains untested. The model also handles recognition of formats such as Markdown (MD) and LaTeX.

**ä¸­æ–‡ï¼š**  
è¯¥æ¨¡å‹ä¸“ä¸ºæHippRAGå–ä¸­æ–‡ä¸‰å…ƒç»„ï¼ˆä¸»ä½“-è°“è¯-å®¢ä½“ï¼‰ï¼Œä»…ä½¿ç”¨ä¸­æ–‡è¯­æ–™è¿›è¡Œè®­ç»ƒï¼Œä½†å¯é€šè¿‡æä¾›çš„è®­ç»ƒä»£ç æ‰©å±•è‡³æ¨¡æ‹Ÿå…¶ä»–è¯­è¨€ã€‚åŸºäºQwen3-1.7Bï¼Œå¯æ‰©å±•è‡³Qwenç³»åˆ—çš„å…¶ä»–æ¨¡å‹ï¼›ä¸å…¶ä»–æ¨¡å‹æ—çš„å…¼å®¹æ€§å°šæœªéªŒè¯ã€‚å¯æ”¯æŒMarkdownï¼ˆMDï¼‰ã€LaTeXç­‰æ•°æ®æ ¼å¼çš„è¯†åˆ«ã€‚

## Usage / ä½¿ç”¨

**English:**  
The invocation method aligns with Qwen3 (refer to [Qwen3 Documentation](https://huggingface.co/Qwen)). Due to partial incompatibility of Transformers with certain inference environments, generation may continue indefinitely; it is advisable to incorporate a stop token like `"}]}` as a safeguard.

**ä¸­æ–‡ï¼š**  
è°ƒç”¨æ–¹å¼ä¸Qwen3ç›¸åŒï¼ˆå‚è§[Qwen3æ–‡æ¡£](https://huggingface.co/Qwen)ï¼‰ã€‚ç”±äºTransformersä¸éƒ¨åˆ†æ¨ç†ç¯å¢ƒçš„ä¸å®Œå…¨å…¼å®¹ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆæ— ä¼‘æ­¢ï¼Œå»ºè®®æ·»åŠ åœæ­¢ç¬¦`"}]}`ä½œä¸ºåŒé‡ä¿éšœã€‚

## Training / è®­ç»ƒ

**English:**  
Given the lack of provided data, the model's performance is moderate. You can enhance it through further training using the code below (involving two datasets: one simple and one challenging).

**ä¸­æ–‡ï¼š**  
ç”±äºç¼ºä¹å¤–éƒ¨æ•°æ®æ”¯æŒï¼Œæ¨¡å‹æ•ˆæœä¸­ç­‰ã€‚å¯ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œå¢é‡è®­ç»ƒï¼ˆæ¶‰åŠä¸¤ä¸ªæ•°æ®é›†ï¼šä¸€ä¸ªç®€å•ï¼Œä¸€ä¸ªè¾ƒå¤æ‚ï¼‰ã€‚

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Windows ç‰ˆï¼šè‡ªå®šä¹‰å¤åˆæŸå¤± + 5è½®è¯¾ç¨‹å­¦ä¹ ï¼ˆç®€å•â†’å¤æ‚ï¼‰
- å…¼å®¹ Windows è·¯å¾„ï¼ˆPathlibï¼‰
- é¿å… Windows DataLoader å¤šè¿›ç¨‹é—®é¢˜ï¼ˆnum_workers=0ï¼‰
- è‡ªåŠ¨è¡¥ pad_token_id
- device_map="auto"ï¼ˆæœ‰ CUDA åˆ™èµ° GPUï¼‰
"""

import os
import json
import math
import torch
import torch.nn.functional as F
from datetime import datetime
from tqdm import tqdm
from torch.optim import AdamW
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset, concatenate_datasets
from accelerate import Accelerator
from pathlib import Path

# ========== ç¯å¢ƒå»ºè®® ==========
# Windows ä¸Šå»ºè®®æ˜¾å¼å…³é—­å¤šæ ¸åˆ†è¯çš„çº¿ç¨‹æç¤º
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

# ========== å…¨å±€é…ç½®ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ ==========
# æ¨¡å‹è·¯å¾„ï¼ˆWindows ç¤ºä¾‹ï¼‰
MODEL_PATH  = r"H:\model\qwen3"

# è®­ç»ƒæ•°æ®ï¼ˆæŠŠè¿™ä¸¤ä¸ªæ”¹æˆä½ çš„æœ¬æœºè·¯å¾„ï¼‰
TRAIN_FILE  = r"H:\data\train_fixed.json"
PARA_FILE   = r"H:\data\paragraph_train.json"

# è¾“å‡ºç›®å½•ï¼ˆæ—¶é—´æˆ³ï¼‰
OUTPUT_ROOT = Path(r"H:\model") / f"qwen3_custom_ft_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)

print("ğŸš€ å½“å‰å¯è§ GPU æ•°é‡:", torch.cuda.device_count())

# ========== ä¸»æµç¨‹ ==========
def step4_with_curriculum():
    print("=== Step4: è‡ªå®šä¹‰å¤åˆæŸå¤± + 5è½®è¯¾ç¨‹å­¦ä¹ ï¼ˆWindows ç‰ˆï¼‰ ===")
    out_dir = OUTPUT_ROOT / "step4_custom_curriculum"
    out_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½åˆ†è¯å™¨ä¸æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token_id is None:
        # æ²¡æœ‰ pad_token å°±ç”¨ eos å…œåº•
        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token is not None else "</s>"

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        device_map="auto"  # æœ‰ CUDA ä¼šè‡ªåŠ¨æ”¾ GPU
    )
    model.train()
    for p in model.parameters():
        if torch.is_floating_point(p):
            p.requires_grad = True

    # ========== æ•°æ®å‡†å¤‡ï¼šsimple_raw / complex_rawï¼ˆå†åˆ‡ä¸¤åŠï¼‰==========
    # Windows è·¯å¾„ç”¨å­—ç¬¦ä¸²å³å¯ï¼Œdatasets å†…éƒ¨å…¼å®¹
    orig_ds = load_dataset("json", data_files={"orig": str(TRAIN_FILE)})["orig"]
    para_ds = load_dataset("json", data_files={"para": str(PARA_FILE)})["para"].shuffle(seed=42)

    half = (len(para_ds) - len(orig_ds)) // 2 if (len(para_ds) > len(orig_ds)) else len(para_ds) // 2
    simple_raw  = concatenate_datasets([orig_ds, para_ds.select(range(half))])
    complex_raw = para_ds.select(range(half, len(para_ds)))

    # å°† complex å†åˆ‡ä¸¤åŠï¼šc1 / c2
    c_half = len(complex_raw) // 2
    complex1_raw = complex_raw.select(range(c_half))
    complex2_raw = complex_raw.select(range(c_half, len(complex_raw)))
    complex_all_raw = concatenate_datasets([complex1_raw, complex2_raw])
    all_raw = concatenate_datasets([simple_raw, complex_all_raw])

    # ========== Prompt æ„é€  & é¢„å¤„ç† ==========
    INSTR = (
        "è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–ä¸‰å…ƒç»„ï¼Œè¾“å‡ºæ ¼å¼ä¸ºæ ‡å‡†JSONæ•°ç»„ï¼š\n"
        "è¯·åŠ¡å¿…ä¸¥æ ¼è¾“å‡ºJSONï¼Œä¸è¦é™„åŠ è¯´æ˜æ–‡å­—ã€‚\n"
        "å­—æ®µ: subject=ä¸»ä½“, predicate=å…³ç³», object=å®¢ä½“ï¼›è¯·å°½å¯èƒ½æå–æ‰€æœ‰ç›¸å…³å…³ç³»ä¸”ä¸è¦æ··æ·†ä¸»ä½“ä¸å®¢ä½“ã€‚\n\n"
    )
    def build_prompt(text):
        return f"<|user|>\n{INSTR}{text}\n<|assistant|>\n"

    MAX_LEN = 1024

    def preprocess(ex):
        # å…¼å®¹ input/output å¯èƒ½æ˜¯éå­—ç¬¦ä¸²çš„æƒ…å†µ
        src_inp = ex.get("input", "")
        tgt_out = ex.get("output", "")
        if not isinstance(src_inp, str):
            src_inp = str(src_inp)
        if not isinstance(tgt_out, str):
            tgt_out = json.dumps(tgt_out, ensure_ascii=False)

        prompt = build_prompt(src_inp)
        full   = prompt + tgt_out

        tok    = tokenizer(
            full,
            max_length=MAX_LEN,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        ids  = tok.input_ids[0]
        mask = tok.attention_mask[0]
        labels = ids.clone()

        # è®¡ç®— prompt é•¿åº¦ï¼Œå±è”½å…¶ loss
        plen = tokenizer(prompt, return_tensors="pt").input_ids.size(1)
        labels[:plen] = -100

        # predicate æ©ç ï¼ˆæœ´ç´  token åŒ¹é…ï¼‰
        pmask = torch.zeros_like(ids, dtype=torch.bool)
        try:
            # è¿™é‡Œ ex["output"] è‹¥ä¸æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œä¼šåœ¨ä¸Šé¢æ”¹æˆå­—ç¬¦ä¸²
            preds = [t["predicate"] for t in json.loads(tgt_out)]
            tokens = tokenizer.convert_ids_to_tokens(ids)
            for pred in preds:
                toks = tokenizer.tokenize(pred)
                L = len(toks)
                if L == 0:
                    continue
                for i in range(len(tokens) - L + 1):
                    if tokens[i:i+L] == toks:
                        pmask[i:i+L] = True
        except Exception:
            pass

        return {
            "input_ids": ids,
            "attention_mask": mask,
            "labels": labels,
            "predicate_mask": pmask
        }

    accel = Accelerator()

    # Windows ä¸Š datasets çš„ map é»˜è®¤å•è¿›ç¨‹å³å¯ï¼ˆé¿å…å¤šè¿›ç¨‹ spawn éº»çƒ¦ï¼‰
    with accel.main_process_first():
        simple      = simple_raw.map(preprocess, remove_columns=simple_raw.column_names)
        complex1    = complex1_raw.map(preprocess, remove_columns=complex1_raw.column_names)
        complex2    = complex2_raw.map(preprocess, remove_columns=complex2_raw.column_names)
        complex_all = complex_all_raw.map(preprocess, remove_columns=complex_all_raw.column_names)
        all_ds      = all_raw.map(preprocess, remove_columns=all_raw.column_names)

    for ds in (simple, complex1, complex2, complex_all, all_ds):
        ds.set_format(type="torch", columns=["input_ids", "attention_mask", "labels", "predicate_mask"])

    # DataLoaderï¼šWindows ä¸‹ç¨³å¦¥ç”¨å•è¿›ç¨‹
    bs = 4
    num_workers = 0  # â˜… Windowsï¼š0 æœ€ç¨³ï¼Œé¿å…å¤šè¿›ç¨‹å¡æ­»
    dl_args = dict(batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=torch.cuda.is_available())

    simple_loader      = DataLoader(simple,      **dl_args)
    complex1_loader    = DataLoader(complex1,    **dl_args)
    complex2_loader    = DataLoader(complex2,    **dl_args)
    complex_all_loader = DataLoader(complex_all, **dl_args)
    all_loader         = DataLoader(all_ds,      **dl_args)

    optimizer = AdamW(model.parameters(), lr=5e-5)
    (model, optimizer,
     simple_loader, complex1_loader, complex2_loader, complex_all_loader, all_loader
    ) = accel.prepare(model, optimizer,
                      simple_loader, complex1_loader, complex2_loader, complex_all_loader, all_loader)

    # ========== è®­ç»ƒå‚æ•° ==========
    alpha, beta, delta = 1.0, 1.0, 0.2
    grad_accum = 4
    rounds = 8  # å›ºå®š 8 è½®ï¼ˆä½ åŸæ³¨é‡Šå†™ 5 è½®ï¼Œä»£ç é‡Œæ˜¯ 8ï¼Œæˆ‘ä¿æŒ 8ï¼‰

    # ========== è®­ç»ƒå­æµç¨‹ ==========
    def train_progressive_mix(loader_s, loader_c, round_idx):
        """ç¬¬1è½®ï¼šç®€å•â†’å¤æ‚æ¦‚ç‡çº¿æ€§ä¸Šå‡"""
        total_steps = max(len(loader_s), len(loader_c))
        it_s, it_c = iter(loader_s), iter(loader_c)
        total_loss, step_count = 0.0, 0
        for step in tqdm(range(total_steps), desc=f"Round {round_idx+1} (progressive mix)"):
            p = (step + 1) / total_steps
            pick_complex = torch.rand(1).item() < p
            if pick_complex:
                try:
                    batch = next(it_c)
                except StopIteration:
                    it_c = iter(loader_c)
                    batch = next(it_c)
            else:
                try:
                    batch = next(it_s)
                except StopIteration:
                    it_s = iter(loader_s)
                    batch = next(it_s)

            loss = compute_loss(model, batch, tokenizer, alpha, beta-0.5, delta)
            accel.backward(loss)
            if (step + 1) % grad_accum == 0:
                accel.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step(); optimizer.zero_grad()
            total_loss += loss.item()
            step_count += 1
        return total_loss, step_count

    def train_uniform_among_loaders(loaders, round_idx):
        """ç¬¬2/3è½®ï¼šæŒ‰æ•°æ®æºå‡ç­‰é‡‡æ ·ï¼ˆè½®è½¬ï¼‰"""
        k = len(loaders)
        max_len = max(len(l) for l in loaders)
        steps = max_len * k
        iters = [iter(l) for l in loaders]
        total_loss, step_count = 0.0, 0

        for step in tqdm(range(steps), desc=f"Round {round_idx+1} (uniform across {k} sources)"):
            idx = step % k
            try:
                batch = next(iters[idx])
            except StopIteration:
                iters[idx] = iter(loaders[idx])
                batch = next(iters[idx])

            loss = compute_loss(model, batch, tokenizer, alpha, beta-0.3, delta)
            accel.backward(loss)
            if (step + 1) % grad_accum == 0:
                accel.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step(); optimizer.zero_grad()
            total_loss += loss.item()
            step_count += 1
        return total_loss, step_count

    def train_single_loader(loader, round_idx):
        """ç¬¬4/5+è½®ï¼šå…¨é‡é¡ºåºè®­ç»ƒ"""
        total_loss, step_count = 0.0, 0
        for step, batch in enumerate(tqdm(loader, desc=f"Round {round_idx+1} (full data)")):
            loss = compute_loss(model, batch, tokenizer, alpha, beta, delta)
            accel.backward(loss)
            if (step + 1) % grad_accum == 0:
                accel.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step(); optimizer.zero_grad()
            total_loss += loss.item()
            step_count += 1
        return total_loss, step_count

    # ========== äº”ï¼ˆå…«ï¼‰è½®è¯¾ç¨‹å­¦ä¹  ==========
    for r in range(rounds):
        if r == 0:
            tot, cnt = train_progressive_mix(simple_loader, complex_all_loader, r)
        elif r == 1:
            tot, cnt = train_uniform_among_loaders([simple_loader, complex1_loader], r)
        elif r == 2:
            tot, cnt = train_uniform_among_loaders([simple_loader, complex1_loader, complex2_loader], r)
        else:
            tot, cnt = train_single_loader(all_loader, r)

        avg_loss = tot / max(1, cnt)
        print(f"âœ… Round {r+1} avg loss: {avg_loss:.4f}")

    # ä¿å­˜
    if accel.is_main_process:
        unwrapped = accel.unwrap_model(model)
        unwrapped.save_pretrained(str(out_dir), safe_serialization=True)
        tokenizer.save_pretrained(str(out_dir))
    print("ğŸ’¾ ä¿å­˜è‡³", out_dir)

# ========== æŸå¤±å‡½æ•° ==========
def compute_loss(model, batch, tokenizer, alpha, beta, delta):
    outputs = model(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        labels=batch["labels"]
    )
    ce_loss = outputs.loss

    # F1 on predicate tokensï¼ˆembedding è¿‘ä¼¼ P/Rï¼‰
    pred_ids = outputs.logits.argmax(dim=-1)
    mask_flat = batch["predicate_mask"].view(-1)
    labels_flat = batch["labels"].view(-1)
    pred_flat = pred_ids.view(-1)
    valid_idx = mask_flat.nonzero(as_tuple=True)[0]

    if valid_idx.numel() > 0:
        true_ids = labels_flat[valid_idx]
        pred_sel = pred_flat[valid_idx]
        emb = model.get_input_embeddings()
        vocab_sz = emb.num_embeddings
        legal = (
            (true_ids >= 0) & (true_ids < vocab_sz) &
            (pred_sel >= 0) & (pred_sel < vocab_sz)
        )
        if legal.sum() > 0:
            true_ids = true_ids[legal]
            pred_sel = pred_sel[legal]
            t_emb = emb(true_ids)
            p_emb = emb(pred_sel)
            S = F.cosine_similarity(t_emb.unsqueeze(1), p_emb.unsqueeze(0), dim=-1)
            P_val = S.max(dim=1).values.mean()
            R_val = S.max(dim=0).values.mean()
            F1 = 2 * P_val * R_val / (P_val + R_val + 1e-8)
        else:
            F1 = torch.tensor(1.0, device=ce_loss.device)
    else:
        F1 = torch.tensor(1.0, device=ce_loss.device)

    # éç»“æ„è¾“å‡ºæƒ©ç½š
    illegal = (batch["labels"] == -100) & (pred_ids != tokenizer.pad_token_id)
    x = illegal.sum().float().clamp(min=0.0)
    penalty = 1.0 - 1.0 / torch.log(x + 10.0)

    return alpha * ce_loss + beta * (1 - F1) + delta * penalty

# ========== å…¥å£ ==========
if __name__ == "__main__":
    # Windows ä¸‹æ¨èï¼š
    # 1) Python 3.10/3.11 + torch/cu ç‰ˆæœ¬åŒ¹é…
    # 2) å…ˆæŠŠ TRAIN_FILE / PARA_FILE æ”¹æˆä½ çš„çœŸå®è·¯å¾„
    step4_with_curriculum()
print("ğŸ‰ å®Œæˆ")
```

## Training Logic / è®­ç»ƒé€»è¾‘

**English:**  
The training adopts a curriculum learning strategy across 8 rounds, incorporating a composite loss function. Denote the simple dataset as $D_s$, the halves of the complex dataset as $D_{c1}$ and $D_{c2}$, with $D_c = D_{c1} \cup D_{c2}$, and $D_a = D_s \cup D_c$.

- **Round 1:** Progressive mixing: For each step $t = 1$ to $T = \max(|D_s|, |D_c|)$, sample from $D_c$ with probability $p_t = t / T$, otherwise from $D_s$. Loss: $L = \alpha \cdot L_{CE} + (\beta - 0.5) \cdot (1 - F1_p) + \delta \cdot P$, where $L_{CE}$ is cross-entropy loss, $F1_p$ approximates the F1-score on predicate tokens using cosine similarity of embeddings, and $P = 1 - 1 / \log(x + 10)$ penalizes non-structured outputs with $x$ being the count of illegal tokens.
- **Round 2:** Uniform sampling across $\{D_s, D_{c1}\}$: Cycle through loaders for $T = 2 \cdot \max(|D_s|, |D_{c1}|)$ steps, using $\beta - 0.3$.
- **Round 3:** Uniform across $\{D_s, D_{c1}, D_{c2}\}$: Similarly, $T = 3 \cdot \max$ over the three, using $\beta - 0.3$.
- **Rounds 4-8:** Full sequential training on $D_a$, employing the full $\beta$.

Optimization: AdamW with learning rate $5 \times 10^{-5}$, gradient accumulation every 4 steps, and clipping at 1.0. Parameters: $\alpha=1.0$, $\beta=1.0$, $\delta=0.2$.

**ä¸­æ–‡ï¼š**  
è®­ç»ƒé‡‡ç”¨8è½®è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆå¤åˆæŸå¤±å‡½æ•°ã€‚è®¾ç®€å•æ•°æ®é›†ä¸º$D_s$ï¼Œå¤æ‚æ•°æ®é›†çš„ä¸¤åŠä¸º$D_{c1}$å’Œ$D_{c2}$ï¼Œ$D_c = D_{c1} \cup D_{c2}$ï¼Œ$D_a = D_s \cup D_c$ã€‚

- **ç¬¬1è½®ï¼š** æ¸è¿›æ··åˆï¼šå¯¹äºæ¯ä¸ªæ­¥$t = 1$åˆ°$T = \max(|D_s|, |D_c|)$ï¼Œä»¥æ¦‚ç‡$p_t = t / T$ä»$D_c$é‡‡æ ·ï¼Œå¦åˆ™ä»$D_s$ã€‚æŸå¤±ï¼š$L = \alpha \cdot L_{CE} + (\beta - 0.5) \cdot (1 - F1_p) + \delta \cdot P$ï¼Œå…¶ä¸­$L_{CE}$ä¸ºäº¤å‰ç†µæŸå¤±ï¼Œ$F1_p$é€šè¿‡åµŒå…¥ä½™å¼¦ç›¸ä¼¼åº¦è¿‘ä¼¼è°“è¯tokençš„F1åˆ†æ•°ï¼Œ$P = 1 - 1 / \log(x + 10)$æƒ©ç½šéç»“æ„è¾“å‡ºï¼ˆ$x$ä¸ºéæ³•tokenæ•°ï¼‰ã€‚
- **ç¬¬2è½®ï¼š** åœ¨$\{D_s, D_{c1}\}$ä¸Šå‡åŒ€é‡‡æ ·ï¼šå¾ªç¯åŠ è½½å™¨$T = 2 \cdot \max(|D_s|, |D_{c1}|)$æ­¥ï¼Œä½¿ç”¨$\beta - 0.3$ã€‚
- **ç¬¬3è½®ï¼š** åœ¨$\{D_s, D_{c1}, D_{c2}\}$ä¸Šå‡åŒ€é‡‡æ ·ï¼šç±»ä¼¼ï¼Œ$T = 3 \cdot \max$ä¸‰è€…ï¼Œä½¿ç”¨$\beta - 0.3$ã€‚
- **ç¬¬4-8è½®ï¼š** åœ¨$D_a$ä¸Šå…¨é‡é¡ºåºè®­ç»ƒï¼Œä½¿ç”¨å®Œæ•´$\beta$ã€‚

ä¼˜åŒ–ï¼šAdamWï¼Œå­¦ä¹ ç‡$5 \times 10^{-5}$ï¼Œæ¯4æ­¥æ¢¯åº¦ç´¯ç§¯ï¼Œè£å‰ª1.0ã€‚å‚æ•°ï¼š$\alpha=1.0$ï¼Œ$\beta=1.0$ï¼Œ$\delta=0.2$ã€‚
